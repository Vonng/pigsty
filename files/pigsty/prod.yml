---
#==============================================================#
# File      :   prod.yml
# Desc      :   Pigsty config for a prod simulation env on a BM
# Ctime     :   2023-07-20
# Mtime     :   2023-07-30
# Docs      :   https://vonng.github.io/pigsty/#/CONFIG
# Author    :   Ruohang Feng (rh@vonng.com)
# License   :   AGPLv3
#==============================================================#

#==============================================================#
# 10.10.10.1  <------- pg-dell-1  the bare metal 72C 256G 4T SSD
# Vagrantfile: vagrant/spec/dell.rb
#==============================================================#
all:

  children:


    #==========================================================#
    # infra: 2 nodes
    #==========================================================#
    # ./infra.yml -l infra
    infra:
      hosts:
        10.10.10.10: { infra_seq: 1 ,nodename: infra-1 }
        10.10.10.11: { infra_seq: 2 ,nodename: infra-2 }
      vars:
        node_id_from_pg: false
        node_cluster: infra
        node_conf: oltp
        docker_enabled: true

    #==========================================================#
    # node: 42 nodes 71C-174G
    #==========================================================#
    # ./node.yml
    nodes:
      hosts:
        10.10.10.10 : { nodename: meta-1        ,cpu: 8 ,mem: 32768  ,image: generic/rocky9 }
        10.10.10.11 : { nodename: meta-2        ,cpu: 8 ,mem: 32768  ,image: generic/rocky9 }
        10.10.10.12 : { nodename: pg-v12-1      ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.13 : { nodename: pg-v13-1      ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.14 : { nodename: pg-v14-1      ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.15 : { nodename: pg-v15-1      ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.16 : { nodename: pg-v16-1      ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.21 : { nodename: etcd-1        ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.22 : { nodename: etcd-2        ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.23 : { nodename: etcd-3        ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.24 : { nodename: etcd-4        ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.25 : { nodename: etcd-5        ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.26 : { nodename: minio-1       ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.27 : { nodename: minio-2       ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.28 : { nodename: minio-3       ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.29 : { nodename: proxy-1       ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.30 : { nodename: proxy-2       ,cpu: 2 ,mem: 4096   ,image: generic/rocky9 }
        10.10.10.41 : { nodename: pg-test-1     ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.42 : { nodename: pg-test-2     ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.43 : { nodename: pg-test-3     ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.44 : { nodename: pg-test-4     ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.45 : { nodename: pg-src-1      ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.46 : { nodename: pg-src-2      ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.47 : { nodename: pg-src-3      ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.48 : { nodename: pg-dst-1      ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.49 : { nodename: pg-dst-2      ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.50 : { nodename: pg-citus0-1   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.51 : { nodename: pg-citus0-2   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.52 : { nodename: pg-citus1-1   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.53 : { nodename: pg-citus1-2   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.54 : { nodename: pg-citus2-1   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.55 : { nodename: pg-citus2-2   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.56 : { nodename: pg-citus3-1   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.57 : { nodename: pg-citus3-2   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.58 : { nodename: pg-citus4-1   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.59 : { nodename: pg-citus4-2   ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.81 : { nodename: redis-test-1  ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.82 : { nodename: redis-test-2  ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.83 : { nodename: redis-test-3  ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.84 : { nodename: redis-test-4  ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.85 : { nodename: redis-ms-1    ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }
        10.10.10.86 : { nodename: redis-ms-2    ,cpu: 1 ,mem: 2048   ,image: generic/rocky9 }


    #==========================================================#
    # pg-meta: reuse infra node as meta cmdb
    #==========================================================#
    # ./pgsql.yml -l pg-meta
    pg-meta:
      hosts:
        10.10.10.10: { pg_seq: 1 , pg_role: primary }
        10.10.10.11: { pg_seq: 2 , pg_role: replica }
      vars:
        pg_cluster: pg-meta
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.2/24
        pg_vip_interface: eth1
        pg_users:
          - { name: dbuser_meta ,password: DBUser.Meta   ,pgbouncer: true ,roles: [ dbrole_admin ]    ,comment: pigsty admin user }
          - { name: dbuser_view ,password: DBUser.Viewer ,pgbouncer: true ,roles: [ dbrole_readonly ] ,comment: read-only viewer for meta database }
        pg_databases:
          - { name: meta ,baseline: cmdb.sql ,comment: pigsty meta database ,schemas: [ pigsty ] ,extensions: [ { name: postgis, schema: public }, { name: timescaledb } ] }
        pg_hba_rules:
          - { user: dbuser_view , db: all ,addr: infra ,auth: pwd ,title: 'allow grafana dashboard access cmdb from infra nodes' }
        node_crontab:  # make a full backup on monday 1am, and an incremental backup during weekdays
          - '00 01 * * 1 postgres /pg/bin/pg-backup full'
          - '00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup'

    #==========================================================#
    # pg-v12 - v16
    #==========================================================#
    # ./pgsql.yml -l pg-v*
    pg-v12:
      hosts: { 10.10.10.12: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v12
        pg_version: 12
        pg_libs: 'pg_stat_statements, auto_explain'
        pg_packages: [ 'postgresql12* wal2json_12* pg_repack_12* passwordcheck_cracklib_12* pgbouncer pg_exporter pgbadger vip-manager patroni patroni-etcd pgbackrest' ]
        pg_extensions: []
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v12 } ]
        pg_service_provider: proxy       # use load balancer on group `proxy` with port 10012
        pg_default_services:  [{ name: primary ,port: 10012 ,dest: postgres  ,check: /primary   ,selector: "[]" }]


    pg-v13:
      hosts: { 10.10.10.13: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v13
        pg_version: 13
        pg_libs: 'pg_stat_statements, auto_explain'
        pg_packages: [ 'postgresql13* wal2json_13* pg_repack_13* passwordcheck_cracklib_13* pgbouncer pg_exporter pgbadger vip-manager patroni patroni-etcd pgbackrest' ]
        pg_extensions: []
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v13 } ]
        pg_service_provider: proxy       # use load balancer on group `proxy` with port 10013
        pg_default_services:  [{ name: primary ,port: 10013 ,dest: postgres  ,check: /primary   ,selector: "[]" }]

    pg-v14:
      hosts: { 10.10.10.14: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v14
        pg_version: 14
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v14 } ]
        pg_service_provider: proxy       # use load balancer on group `proxy` with port 10014
        pg_default_services:  [{ name: primary ,port: 10014 ,dest: postgres  ,check: /primary   ,selector: "[]" }]

    pg-v15:
      hosts: { 10.10.10.15: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v15
        pg_version: 15
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v15 } ]
        pg_service_provider: proxy       # use load balancer on group `proxy` with port 10013
        pg_default_services:  [{ name: primary ,port: 10015 ,dest: postgres  ,check: /primary   ,selector: "[]" }]

    pg-v16:
      hosts: { 10.10.10.16: { pg_seq: 1 ,pg_role: primary } }
      vars:
        pg_cluster: pg-v16
        pg_version: 16
        pg_libs: 'pg_stat_statements, auto_explain'
        pg_packages: [ postgresql16* pgbouncer pg_exporter pgbadger vip-manager patroni patroni-etcd pgbackrest ]
        pg_extensions: [ ]
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: v16 } ]
        pg_service_provider: proxy       # use load balancer on group `proxy` with port 10013
        pg_default_services:  [{ name: primary ,port: 10016 ,dest: postgres  ,check: /primary   ,selector: "[]" }]


    #==========================================================#
    # etcd: 5 nodes used as dedicated minio cluster
    #==========================================================#
    # ./etcd.yml -l etcd;
    etcd:  # the 5 node etcd dcs cluster
      hosts:
        10.10.10.21: { etcd_seq: 1 , nodename: etcd-1 }
        10.10.10.22: { etcd_seq: 2 , nodename: etcd-2 }
        10.10.10.23: { etcd_seq: 3 , nodename: etcd-3 }
        10.10.10.24: { etcd_seq: 4 , nodename: etcd-4 }
        10.10.10.25: { etcd_seq: 5 , nodename: etcd-5 }
      vars:
        etcd_cluster: etcd
        node_cluster: etcd


    #==========================================================#
    # minio: 3 nodes used as dedicated minio cluster
    #==========================================================#
    # ./minio.yml -l minio;
    minio:   # access service via sss.pigsty:9002
      hosts:
        10.10.10.26: { minio_seq: 1 , nodename: minio-1 }
        10.10.10.27: { minio_seq: 2 , nodename: minio-2 }
        10.10.10.28: { minio_seq: 3 , nodename: minio-3 }
      vars:
        minio_cluster: minio
        node_cluster: minio
        minio_data: '/data'                 # it should be a raw device for a multi-node deployment!
        minio_node: '${minio_cluster}-${minio_seq}.pigsty' # minio node name pattern
        haproxy_services:                   # expose minio service via haproxy node port 9002
          - name: minio                     # [REQUIRED] service name, unique
            port: 9002                      # [REQUIRED] service port, unique
            options:                        # [OPTIONAL] minio health check
              - option httpchk
              - option http-keep-alive
              - http-check send meth OPTIONS uri /minio/health/live
              - http-check expect status 200
            servers:
              - { name: minio-1 ,ip: 10.10.10.26 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' }
              - { name: minio-2 ,ip: 10.10.10.27 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' }
              - { name: minio-3 ,ip: 10.10.10.28 ,port: 9000 ,options: 'check-ssl ca-file /etc/pki/ca.crt check port 9000' }


    #==========================================================#
    # proxy: 2 nodes used as dedicated haproxy server
    #==========================================================#
    # ./node.yml -l proxy
    proxy:
      hosts:
        10.10.10.29: { nodename: proxy-1 }
        10.10.10.30: { nodename: proxy-2 }
      vars:
        node_cluster: proxy


    #==========================================================#
    # pg-test: dedicate 4 node testing cluster
    #==========================================================#
    # ./pgsql.yml -l pg-test
    pg-test:
      hosts:
        10.10.10.41: { pg_seq: 1 ,pg_role: primary }
        10.10.10.42: { pg_seq: 2 ,pg_role: replica }
        10.10.10.43: { pg_seq: 3 ,pg_role: replica }
        10.10.10.44: { pg_seq: 4 ,pg_role: replica }
      vars:
        pg_cluster: pg-test
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.3/24
        pg_vip_interface: eth1
        pg_users:  [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }]
        pg_databases: [{ name: test }]


    #==========================================================#
    # pg-src: dedicate 3 node testing cluster, version 14
    #==========================================================#
    # ./pgsql.yml -l pg-src
    pg-src:
      hosts:
        10.10.10.45: { pg_seq: 1 ,pg_role: primary }
        10.10.10.46: { pg_seq: 2 ,pg_role: replica }
        10.10.10.47: { pg_seq: 3 ,pg_role: replica }
      vars:
        pg_cluster: pg-src
        pg_version: 14
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.4/24
        pg_vip_interface: eth1
        node_hugepage_ratio: 0.3
        pg_users:  [{ name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] }]
        pg_databases: [{ name: src }]


    #==========================================================#
    # pg-dst: dedicate 2 node testing cluster
    #==========================================================#
    # ./pgsql.yml -l pg-dst
    pg-dst:
      hosts:
        10.10.10.48: { pg_seq: 1 ,pg_role: primary } # 8C 8G
        10.10.10.49: { pg_seq: 2 ,pg_role: replica } # 1C 2G
      vars:
        pg_cluster: pg-dst
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.5/24
        pg_vip_interface: eth1
        pg_users: [ { name: test , password: test , pgbouncer: true , roles: [ dbrole_admin ] } ]
        pg_databases: [ { name: dst } ]


    #==========================================================#
    # pg-citus: 10 node citus cluster (5 x 1p1s)
    #==========================================================#
    pg-citus0: # citus coordinator, pg_group = 0
      hosts:
        10.10.10.50: { pg_seq: 1, pg_role: primary }
        10.10.10.51: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus0
        pg_group: 0
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: test            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.60/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: test ,password: test ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: test ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus1: # citus data node 1, pg_group = 1
      hosts:
        10.10.10.52: { pg_seq: 1, pg_role: primary }
        10.10.10.53: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus1
        pg_group: 1
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: test            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.61/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: test ,password: test ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: test ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus2: # citus data node 2, pg_group = 2
      hosts:
        10.10.10.54: { pg_seq: 1, pg_role: primary }
        10.10.10.55: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus2
        pg_group: 2
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: test            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.62/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: test ,password: test ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: test ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus3: # citus data node 3, pg_group = 3
      hosts:
        10.10.10.56: { pg_seq: 1, pg_role: primary }
        10.10.10.57: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus3
        pg_group: 3
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: test            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.63/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: test ,password: test ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: test ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }

    pg-citus4: # citus data node 4, pg_group = 4
      hosts:
        10.10.10.58: { pg_seq: 1, pg_role: primary }
        10.10.10.59: { pg_seq: 2, pg_role: replica }
      vars:
        pg_cluster: pg-citus4
        pg_group: 4
        pg_mode: citus                    # pgsql cluster mode: citus
        pg_shard: pg-citus                # citus shard name: pg-citus
        patroni_citus_db: test            # citus distributed database name
        pg_dbsu_password: DBUser.Postgres # all dbsu password access for citus cluster
        pg_vip_enabled: true
        pg_vip_address: 10.10.10.64/24
        pg_vip_interface: eth1
        pg_libs: 'citus, timescaledb, pg_stat_statements, auto_explain' # citus will be added by patroni automatically
        pg_users: [{ name: test ,password: test ,pgbouncer: true ,roles: [ dbrole_admin ] }]
        pg_databases:  [{ name: test ,owner: citus ,extensions: [{name: citus}, {name: postgis}, {name: timescaledb}] }]
        pg_hba_rules:
          - { user: 'all' ,db: all  ,addr: 10.10.10.0/24 ,auth: trust ,title: 'trust citus cluster members'        }
          - { user: 'all' ,db: all  ,addr: 127.0.0.1/32  ,auth: ssl   ,title: 'all user ssl access from localhost' }
          - { user: 'all' ,db: all  ,addr: intra         ,auth: ssl   ,title: 'all user ssl access from intranet'  }




    #==========================================================#
    # redis-meta: reuse the 5 etcd nodes as redis sentinel
    #==========================================================#
    # ./redis.yml -l redis-meta
    redis-meta:
      hosts:
        10.10.10.21: { redis_node: 1 , redis_instances: { 6001: {} } }
        10.10.10.22: { redis_node: 2 , redis_instances: { 6001: {} } }
        10.10.10.23: { redis_node: 3 , redis_instances: { 6001: {} } }
        10.10.10.24: { redis_node: 4 , redis_instances: { 6001: {} } }
        10.10.10.25: { redis_node: 5 , redis_instances: { 6001: {} } }
      vars:
        redis_cluster: redis-meta
        redis_password: 'redis.meta'
        redis_mode: sentinel
        redis_max_memory: 256MB


    #==========================================================#
    # redis-src: reuse pg-src 3 nodes for redis
    #==========================================================#
    # ./redis.yml -l redis-src
    redis-src:
      hosts:
        10.10.10.85: { redis_node: 1 , redis_instances: {6501: {  }                               }}
        10.10.10.46: { redis_node: 2 , redis_instances: {6501: { replica_of: '10.10.10.45 6501' } }}
        10.10.10.47: { redis_node: 3 , redis_instances: {6501: { replica_of: '10.10.10.45 6501' } }}
      vars:
        redis_cluster: redis-src
        redis_password: 'redis.src'
        redis_max_memory: 128MB

    #==========================================================#
    # redis-dst: reuse pg-dst 2 nodes for redis
    #==========================================================#
    # ./redis.yml -l redis-dst
    redis-dst:
      hosts:
        10.10.10.48: { redis_node: 1 , redis_instances: {6501: {  }                               }}
        10.10.10.49: { redis_node: 2 , redis_instances: {6501: { replica_of: '10.10.10.48 6501' } }}
      vars:
        redis_cluster: redis-dst
        redis_password: 'redis.dst'
        redis_max_memory: 128MB

    #==========================================================#
    # redis-test: redis native cluster in 6 nodes, 24 instances
    #==========================================================#
    # ./node.yml -l redis-test; ./redis.yml -l redis-test
    redis-test:
      hosts:
        10.10.10.81: { redis_node: 1 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.82: { redis_node: 2 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.83: { redis_node: 3 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.84: { redis_node: 4 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.85: { redis_node: 5 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
        10.10.10.86: { redis_node: 6 ,redis_instances: { 6501: {} ,6502: {} ,6503: {} } }
      vars:
        redis_cluster: redis-test
        redis_password: 'redis.test'
        redis_mode: cluster
        redis_max_memory: 512MB
        node_cluster: redis-test


  #============================================================#
  # Global Variables
  #============================================================#
  vars:

    #==========================================================#
    # INFRA
    #==========================================================#
    version: v2.2.0                   # pigsty version string
    admin_ip: 10.10.10.11             # admin node ip address
    region: china                     # upstream mirror region: default|china|europe
    infra_portal:                     # domain names and upstream servers
      home         : { domain: h.pigsty }
      grafana      : { domain: g.pigsty ,endpoint: "10.10.10.10:3000" , websocket: true }
      prometheus   : { domain: p.pigsty ,endpoint: "10.10.10.10:9090" }
      alertmanager : { domain: a.pigsty ,endpoint: "10.10.10.10:9093" }
      blackbox     : { endpoint: "10.10.10.10:9115" }
      loki         : { endpoint: "10.10.10.10:3100" }
      minio        : { domain: sss.pigsty  ,endpoint: "10.10.10.26:9001" ,scheme: https ,websocket: true }
    nginx_navbar: []
    dns_records:                      # dynamic dns records resolved by dnsmasq
      - 10.10.10.1 h.pigsty a.pigsty p.pigsty g.pigsty

    #==========================================================#
    # NODE
    #==========================================================#
    node_conf: tiny # use rocky9 local repo as node default
    node_repo_local_urls: ['http://10.10.10.11/pigsty.repo']
    node_timezone: Asia/Hong_Kong     # use Asia/Hong_Kong Timezone
    node_dns_servers:                 # DNS servers in /etc/resolv.conf
      - 10.10.10.10
      - 10.10.10.11
    node_etc_hosts:
      - 10.10.10.10 h.pigsty a.pigsty p.pigsty g.pigsty
      - 10.10.10.26 sss.pigsty
    node_ntp_servers:                 # NTP servers in /etc/chrony.conf
      - pool cn.pool.ntp.org iburst
      - pool 10.10.10.10 iburst
    docker_registry_mirrors:          # docker mirror in mainland china
      - "https://hub-mirror.c.163.com"
      - "https://mirror.baidubce.com"

    #==========================================================#
    # PGSQL
    #==========================================================#
    pg_conf: tiny.yml
    pgbackrest_method: minio          # USE THE HA MINIO THROUGH A LOAD BALANCER
    pgbackrest_repo:                  # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository
      local:                          # default pgbackrest repo with local posix fs
        path: /pg/backup              # local backup directory, `/pg/backup` by default
        retention_full_type: count    # retention full backups by count
        retention_full: 2             # keep 2, at most 3 full backup when using local fs repo
      minio:
        type: s3
        s3_endpoint: sss.pigsty       # s3_endpoint could be any load balancer: 10.10.10.1{0,1,2}, or domain names point to any of the 3 nodes
        s3_region: us-east-1          # you could use external domain name: sss.pigsty , which resolve to any members  (`minio_domain`)
        s3_bucket: pgsql              # instance & nodename can be used : minio-1.pigsty minio-1.pigsty minio-1.pigsty minio-1 minio-2 minio-3
        s3_key: pgbackrest            # Better using a new password for MinIO pgbackrest user
        s3_key_secret: S3User.Backup
        s3_uri_style: path
        path: /pgbackrest
        storage_port: 9002            # Use the load balancer port 9002 instead of default 9000 (direct access)
        storage_ca_file: /etc/pki/ca.crt
        bundle: y
        cipher_type: aes-256-cbc      # Better using a new cipher password for your production environment
        cipher_pass: pgBackRest.${pg_cluster}
        retention_full_type: time
        retention_full: 14

...